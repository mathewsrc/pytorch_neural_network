{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ9bDw80x5kP5+TI2crfk5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/pytorch_neural_network/blob/master/fine_tunning_covolution_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dPteP_dbCCoc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "try:\n",
        "  import torch\n",
        "  from torch import nn, optim\n",
        "  import torch.nn.functional as F\n",
        "  import torchvision\n",
        "  from torchvision import datasets, transforms, models\n",
        "except:\n",
        "  !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the neural network on the image dataset\n",
        "def train(model, train_loader, validation_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains a neural network on a given image dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        The neural network to be trained\n",
        "    train_loader : DataLoader\n",
        "        The training data in the form of PyTorch DataLoader\n",
        "    validation_loader : DataLoader\n",
        "        The validation data in the form of PyTorch DataLoader\n",
        "    criterion : nn.Module\n",
        "        The loss function used for training\n",
        "    optimizer : torch.optim\n",
        "        The optimizer used for training\n",
        "    device : torch.device\n",
        "        The device to run the training on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        The trained neural network\n",
        "    \"\"\"\n",
        "    # Define number of training epochs\n",
        "    epochs = 2\n",
        "    # Set best loss value to an arbitrary high value\n",
        "    best_loss = 1e6\n",
        "    # Dictionary to store the train and validation data loaders\n",
        "    image_dataset = {'train': train_loader, 'valid': validation_loader}\n",
        "    # Counter to keep track of number of times validation loss increases\n",
        "    loss_counter = 0\n",
        "\n",
        "    # Loop through the epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Loop through train and validation phases\n",
        "        for phase in ['train', 'valid']:\n",
        "            print(f\"Epoch {epoch}, Phase {phase}\")\n",
        "            # Set the model to train mode for train phase and eval mode for validation phase\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            # Initialize variables to keep track of running loss and accuracy\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            running_samples = 0\n",
        "\n",
        "            # Loop through the data in the data loader\n",
        "            for step, (inputs, labels) in enumerate(image_dataset[phase]):\n",
        "                # Transfer inputs and labels to the device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # Get model outputs\n",
        "                outputs = model(inputs)\n",
        "                # Calculate the loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Perform backpropagation and optimization only in train phase\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Get the predicted class from the model outputs\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                # Update running loss and accuracy\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data).item()\n",
        "                running_samples += len(inputs)\n",
        "                # Print the current loss and accuracy every 2000 samples\n",
        "                if running_samples % 2000 == 0:\n",
        "                    accuracy = running_corrects/running_samples\n",
        "                    # Printing the loss and accuracy details every 2000 iterations\n",
        "                    print(\"Images [{}/{} ({:.0f}%)] Loss: {:.2f} Accuracy: {}/{} ({:.2f}%)\".format(\n",
        "                            running_samples,\n",
        "                            len(image_dataset[phase].dataset),\n",
        "                            100.0 * (running_samples / len(image_dataset[phase].dataset)),\n",
        "                            loss.item(),\n",
        "                            running_corrects,\n",
        "                            running_samples,\n",
        "                            100.0*accuracy,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            # NOTE: Uncomment the lines below to train and test on the whole dataset\n",
        "            if running_samples>(0.1*len(image_dataset[phase].dataset)):\n",
        "                break\n",
        "\n",
        "            # Calculating the mean loss and accuracy of the epoch\n",
        "            epoch_loss = running_loss / running_samples\n",
        "            epoch_acc = running_corrects / running_samples\n",
        "\n",
        "            # Saving the best validation loss and checking the loss increase counter\n",
        "            if phase=='valid':\n",
        "              if epoch_loss<best_loss:\n",
        "                best_loss=epoch_loss\n",
        "              else:\n",
        "                loss_counter+=1\n",
        "\n",
        "        # Breaking the epoch loop if the loss increase counter reaches 1\n",
        "        if loss_counter==1:\n",
        "          break\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Hy3cUv3BUoo8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, criterion):\n",
        "    \"\"\"\n",
        "    Test the given model on the test dataset using the test_loader and criterion.\n",
        "    \n",
        "    Parameters:\n",
        "    - model (nn.Module): the model to be tested\n",
        "    - test_loader (DataLoader): the test dataset loader\n",
        "    - criterion (nn.Module): the loss function to be used for testing\n",
        "    \n",
        "    Returns:\n",
        "    - total_loss (float): the average loss on the test dataset\n",
        "    - total_acc (float): the average accuracy on the test dataset\n",
        "    \"\"\"\n",
        "    # set the model to evaluation mode\n",
        "    model.eval() \n",
        "\n",
        "    # running loss for each batch\n",
        "    running_loss = 0 \n",
        "    \n",
        "    # running number of correct predictions for each batch\n",
        "    running_corrects = 0 \n",
        "\n",
        "    # loop through the test_loader\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs) # forward pass through the model\n",
        "        loss = criterion(outputs, labels) # compute the loss\n",
        "        _, preds = torch.max(outputs, 1) # get the predictions by finding the max value in the output\n",
        "        running_loss += loss.item() * inputs.size(0) # accumulate the loss\n",
        "        running_corrects += torch.sum(preds == labels.data).item() # accumulate the number of correct predictions\n",
        "\n",
        "    # calculate the average loss and accuracy on the entire test dataset\n",
        "    total_loss = running_loss / len(test_loader)\n",
        "    total_acc = running_corrects / len(test_loader)\n",
        "    \n",
        "    return total_loss, total_acc\n"
      ],
      "metadata": {
        "id": "fdB6k_jgGr9R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Creates a ResNet18 model with a modified fully connected layer\n",
        "    \n",
        "    Returns:\n",
        "        The ResNet18 model with a modified fully connected layer\n",
        "    \"\"\"\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Freeze all pre-trained parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False   \n",
        "\n",
        "    # Get the number of features from the original fully connected layer\n",
        "    num_features=model.fc.in_features\n",
        "    \n",
        "    # Replace the original fully connected layer with a new layer with 10 output units\n",
        "    model.fc = nn.Sequential(\n",
        "                   nn.Linear(num_features, 10))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kj0DTy0wK7PN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size for training and testing datasets\n",
        "batch_size = 10\n",
        "\n",
        "# Define the transformations for training set\n",
        "training_transform = transforms.Compose([\n",
        "    # Randomly flip the image horizontally with a probability of 0.5\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # Resize the image to 224x224\n",
        "    transforms.Resize(224),\n",
        "    # Convert the image to a Tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the image using the mean and standard deviation for the 3 color channels\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define the transformations for testing set\n",
        "testing_transform = transforms.Compose([\n",
        "    # Convert the image to a Tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Randomly resize and crop the image to 224x224\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    # Normalize the image using the mean and standard deviation for the 3 color channels\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the training set using the defined transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "        download=True, transform=training_transform)\n",
        "\n",
        "# Load the training data into a data loader for batch processing\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "        shuffle=True)\n",
        "\n",
        "# Load the testing set using the defined transformations\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "        download=True, transform=testing_transform)\n",
        "\n",
        "# Load the testing data into a data loader for batch processing\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "        shuffle=False)\n",
        "\n",
        "# Create a model\n",
        "model = create_model()\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on Device {device}\")\n",
        "\n",
        "# Move the model to the specified device\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the criterion (loss function)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer for updating the model parameters\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model using the trainloader, testloader, criterion, and optimizer\n",
        "train(model, train_loader, test_loader, criterion, optimizer, device)\n",
        "\n",
        "# Test the trained model using the test loader and criterion\n",
        "test(model, test_loader, criterion)\n"
      ],
      "metadata": {
        "id": "ar662f2MLn8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}