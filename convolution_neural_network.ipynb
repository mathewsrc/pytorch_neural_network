{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6gyHO4TaJxFAj0tX9DYJ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/pytorch_neural_network/blob/master/convolution_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pytorch - Convolution Neural Network**"
      ],
      "metadata": {
        "id": "dmRTW97DNt1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LeR3E_N4Ne1-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "try:\n",
        "  import torch\n",
        "  from torch import nn, optim\n",
        "  import torch.nn.functional as F\n",
        "  import torchvision\n",
        "  from torchvision import datasets, transforms\n",
        "except:\n",
        "  !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, cost, optimizer, epoch):\n",
        "  \"\"\"\n",
        "    Train a covolution neural network model.\n",
        "    \n",
        "    Parameters:\n",
        "    model (nn.Module): The model to be trained.\n",
        "    train_loader (DataLoader): The training data loader.\n",
        "    cost (function): The cost function used to evaluate the model's performance during training.\n",
        "    optimizer (Optimizer): The optimization algorithm used to update the model's parameters.\n",
        "    epoch (int): The number of iterations over the entire training data.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  # Loop through each epoch\n",
        "  for e in range(epoch):\n",
        "    # Initialize running loss to 0\n",
        "    running_loss = 0\n",
        "    # Initialize the number of correct predictions to 0\n",
        "    correct = 0\n",
        "    # Loop through each batch in the train_loader\n",
        "    for data, target in train_loader:\n",
        "      # Zero the gradients of the model parameters\n",
        "      optimizer.zero_grad()\n",
        "      # Get the predictions from the model\n",
        "      pred = model(data)\n",
        "      # Calculate the loss between the predictions and the target\n",
        "      loss = cost(pred, target)\n",
        "      # Add the loss to the running total of the loss\n",
        "      running_loss += loss\n",
        "      # Compute the gradients of the loss with respect to the model parameters\n",
        "      loss.backward()\n",
        "      # Update the model parameters using the optimizer\n",
        "      optimizer.step()\n",
        "      # Find the index of the maximum prediction for each sample in the batch\n",
        "      pred = pred.argmax(dim=1, keepdim=True)\n",
        "      # Increase the number of correct predictions\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    # Print the average loss and accuracy over the entire training dataset at the end of each epoch\n",
        "    print(f\"Epoch {e}: Loss {running_loss/len(train_loader.dataset)}, Accuracy {100*(correct/len(train_loader.dataset))}%\")"
      ],
      "metadata": {
        "id": "486IA1vlN_MO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    \"\"\"\n",
        "    Test a given model on the test dataset.\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The model to be tested.\n",
        "        test_loader (torch.utils.data.DataLoader): The dataloader for the test data.\n",
        "        \n",
        "    Returns:\n",
        "        None\n",
        "        \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # Calculate the predicted outputs\n",
        "            output = model(data)\n",
        "            # Find the class with maximum probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            # Count the number of correctly classified samples\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Print the accuracy of the model on the test dataset\n",
        "    print(f'Test set: Accuracy: {correct}/{len(test_loader.dataset)} = {100*(correct/len(test_loader.dataset))}%)')"
      ],
      "metadata": {
        "id": "4dRHJHe1P-iX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional neural network model for image classification.\n",
        "    \n",
        "    The model consists of two convolutional layers, followed by three fully-connected layers. \n",
        "    The activation function used is ReLU.\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # First convolutional layer, 3 input channels, 8 output channels, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer with 2x2 window and stride 2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)  # Second convolutional layer, 8 input channels, 24 output channels, 5x5 kernel\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # First fully-connected layer, 120 output units\n",
        "        self.fc2 = nn.Linear(120, 84)  # Second fully-connected layer, 84 output units\n",
        "        self.fc3 = nn.Linear(84, 42)  # Third fully-connected layer, 42 output units\n",
        "        self.fc4 = nn.Linear(42, 10)  # Output layer, 10 output units\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass of the model.\n",
        "        \n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, 32, 32)\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, 10)\n",
        "        \"\"\"\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Apply first convolutional layer and max pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Apply second convolutional layer and max pooling\n",
        "        x = torch.flatten(x, 1)  # Flatten the tensor to 2D\n",
        "        x = F.relu(self.fc1(x))  # Apply first fully-connected layer with ReLU activation\n",
        "        x = F.relu(self.fc2(x))  # Apply second fully-connected layer with ReLU activation\n",
        "        x = F.relu(self.fc3(x))  # Apply third fully-connected layer with ReLU activation\n",
        "        x = self.fc4(x)  # Apply output layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "h_FlG0zaQIWk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting batch size and number of epochs\n",
        "batch_size = 32\n",
        "epoch = 20\n",
        "\n",
        "# Define transforms for training data\n",
        "training_transform = transforms.Compose([\n",
        "    # Apply random horizontal flip with probability 0.5\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # Convert the data to tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the data with mean and standard deviation\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Define transforms for test data\n",
        "testing_transform = transforms.Compose([\n",
        "    # Convert the data to tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the data with mean and standard deviation\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load CIFAR10 training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "        download=True, transform=training_transform)\n",
        "\n",
        "# Create a dataloader for the training dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "        shuffle=True)\n",
        "\n",
        "# Load CIFAR10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "        download=True, transform=testing_transform)\n",
        "\n",
        "# Create a dataloader for the test dataset\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "        shuffle=False)\n",
        "\n",
        "# Create a model instance\n",
        "model=Model()\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "train(model, trainloader, criterion, optimizer, epoch)\n",
        "\n",
        "# Test the model on the test dataset\n",
        "test(model, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uudHX4HQIk-",
        "outputId": "32b6e422-78e3-47da-97c8-74f738b2ff21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 0: Loss 0.07197240740060806, Accuracy 10.042%\n",
            "Epoch 1: Loss 0.06932945549488068, Accuracy 18.358%\n",
            "Epoch 2: Loss 0.060444436967372894, Accuracy 27.548000000000002%\n",
            "Epoch 3: Loss 0.05445527657866478, Accuracy 35.066%\n",
            "Epoch 4: Loss 0.04930206015706062, Accuracy 42.13%\n",
            "Epoch 5: Loss 0.045639656484127045, Accuracy 46.214%\n",
            "Epoch 6: Loss 0.0429612398147583, Accuracy 49.71%\n",
            "Epoch 7: Loss 0.04065779224038124, Accuracy 52.736000000000004%\n",
            "Epoch 8: Loss 0.0387149453163147, Accuracy 55.406%\n",
            "Epoch 9: Loss 0.03718198463320732, Accuracy 57.611999999999995%\n",
            "Epoch 10: Loss 0.035558685660362244, Accuracy 59.53000000000001%\n",
            "Epoch 11: Loss 0.03423401340842247, Accuracy 61.18%\n",
            "Epoch 12: Loss 0.033107057213783264, Accuracy 62.327999999999996%\n",
            "Epoch 13: Loss 0.032091833651065826, Accuracy 63.592000000000006%\n",
            "Epoch 14: Loss 0.03118574060499668, Accuracy 64.834%\n",
            "Epoch 15: Loss 0.03036535158753395, Accuracy 65.648%\n",
            "Epoch 16: Loss 0.02955491654574871, Accuracy 66.744%\n",
            "Epoch 17: Loss 0.02892845682799816, Accuracy 67.306%\n",
            "Epoch 18: Loss 0.02837594412267208, Accuracy 67.99%\n",
            "Epoch 19: Loss 0.027730604633688927, Accuracy 68.65400000000001%\n",
            "Test set: Accuracy: 6736/10000 = 67.36%)\n"
          ]
        }
      ]
    }
  ]
}