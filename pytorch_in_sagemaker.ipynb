{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfm4HNicOF7E39P6zD7734",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/pytorch_neural_network/blob/master/pytorch_in_sagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pytorch in sagemaker**\n",
        "\n",
        "Note: this only works in AWS Sagemaker"
      ],
      "metadata": {
        "id": "XBcJ0CdwOgqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV6z5tHbOYVR"
      },
      "outputs": [],
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "from sagemaker import get_execution_role\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model hyperparameters\n",
        "hyperparameters = {\"epochs\": \"2\", \"batch-size\": \"64\", \"test-batch-size\": \"100\", \"lr\": \"0.001\"}\n",
        "\n",
        "# Create a Pytorch estimator\n",
        "estimator = Pytorch(\n",
        "    entry_point=\"scripts/pytorch_mnist.py\",\n",
        "    base_job_name=\"pytorch_sagemaker\",\n",
        "    role=get_execution_role(),\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    hyperparameters=hyperparameters,\n",
        "    framework_version=\"1.8\",\n",
        "    py_version=\"py36\"\n",
        ")\n",
        "\n",
        "# Train model\n",
        "estimator.fit(wait=True)"
      ],
      "metadata": {
        "id": "Bpn5ZQE0OqrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        self. conv1 = nn. Conv2d(1, 32, 3, 1)\n",
        "        self. conv2 = nn. Conv2d(32, 64, 3, 1)\n",
        "        self. dropout1 : = nn. Dropout(0.25)\n",
        "        self.dropout2 = nn.D .Dropout(0.5)\n",
        "        self.fc1 = nn. Linear(9216, 128)\n",
        "        self.fc2 : = nn. Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x=self.convl(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.conv2(x)\n",
        "        x=F.relu(x)\n",
        "        x=F.max_pool2d(x, 2)\n",
        "        x=self.dropout1(x)\n",
        "        x=torch.flatten(x, 1)\n",
        "        x=self.fc1(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.dropout2(x)\n",
        "        x=self.fc2(x)\n",
        "        output=F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "B6iaM4CIPAZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "i0u40xKtPC9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  \n",
        "            pred = output.argmax(dim=1, keepdim=True) \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "yc24SUFjPFva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "         \"-I-batch-size\",\n",
        "         type=int,\n",
        "         default=64,\n",
        "         metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "     )\n",
        "    parser.add_argument(\n",
        "         \"--test-batch-size\",\n",
        "         type=int,\n",
        "         default=1000,\n",
        "         metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "     )\n",
        "    parser.add_argument(\n",
        "         \"--epochs\",\n",
        "         type=int,\n",
        "         default=14,\n",
        "         metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",)\n",
        "    \n",
        "    parser.add_argument(\"--lr\", type=float, default=1.0, metavar=\"LR\", help=\"learning rate (default: 1.0)\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    \n",
        "    dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(dataset1, train_kwargs**)\n",
        "    test_loader = DataLoader(dataset2, test_kwargs**)\n",
        "\n",
        "    model = Net()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(model, train_loader, optimizer, epoch)\n",
        "        test(model, test_loader)"
      ],
      "metadata": {
        "id": "ybztXLpdPIn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qruD-1VcPKUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}